from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Split JSON File").getOrCreate()

# Read the JSON file into a DataFrame
df = spark.read.json("big_file.json")

# Split the DataFrame into smaller DataFrames, each with a maximum size of 100 MB
split_dfs = df.coalesce(1).repartition(100)

# Save each of the smaller DataFrames as a separate JSON file
for i, split_df in enumerate(split_dfs):
  split_df.write.json("split_file_{}.json".format(i))

spark.stop()
