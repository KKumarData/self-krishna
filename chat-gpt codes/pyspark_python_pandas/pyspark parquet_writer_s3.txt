import boto3
from pyspark.sql import SparkSession

def write_parquet(self, df, bucket, key, mode="overwrite", num_partitions=None, partition_cols=None, compression=None):
    """Write a Spark DataFrame to a Parquet file in an Amazon S3 bucket.

    Parameters:
    - df: Spark DataFrame, the DataFrame to write to the Parquet file
    - bucket: string, the name of the S3 bucket to write the file to
    - key: string, the key of the file in the S3 bucket (e.g. "path/to/file.parquet")
    - mode: string, the write mode for the file (default is "overwrite")
    - num_partitions: int, the number of partitions to use when writing the file (default is None)
    - partition_cols: list of strings, the names of the columns to partition the file by (default is None)
    - compression: string, the compression codec to use when writing the file (default is None)
    """
    try:
        # Set the write mode
        writer = df.write.mode(mode)

        # Set the number of partitions
        if num_partitions:
            writer = writer.partitionBy(num_partitions)
        
        # Set the partition columns
        if partition_cols:
            writer = writer.partitionBy(*partition_cols)
        
        # Set the compression codec
        if compression:
            writer = writer.option("compression", compression)
        
        # Write the DataFrame to a Parquet file in the S3 bucket
        writer.parquet(f"s3://{bucket}/{key}")
		
        print(f"Successfully wrote Parquet file to s3://{bucket}/{key}")
    except Exception as e:
        print(f"Error occurred while writing Parquet file: {e}")




parquet_writer = ParquetWriter()
parquet_writer.write_parquet(df, "my-bucket", "path/to/file.parquet", mode="overwrite", num_partitions=5, partition_cols=["col1", "col2"])



or 

